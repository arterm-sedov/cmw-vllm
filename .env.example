# vLLM Server Configuration
# Default values are optimized for 48GB GPUs (RTX 4090, A6000, etc.)
VLLM_MODEL=Qwen/Qwen3-30B-A3B-Instruct-2507
VLLM_PORT=8000
VLLM_HOST=0.0.0.0
VLLM_MAX_MODEL_LEN=40000  # Reduced from 262144 to fit KV cache in GPU memory
VLLM_GPU_MEMORY_UTILIZATION=0.8  # Leaves headroom for other processes
VLLM_CPU_OFFLOAD_GB=24  # Offloads model weights to CPU RAM for large models
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_ENABLE_AUTO_TOOL_CHOICE=true  # Enable auto tool choice for function calling
VLLM_TOOL_CALL_PARSER=hermesv  # Tool call parser for Qwen models (required for tool calling)

# Model Download Configuration
MODEL_DOWNLOAD_DIR=
HF_CACHE_DIR=

# Logging
LOG_LEVEL=INFO
