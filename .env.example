# vLLM Server Configuration
# Default values are optimized for 48GB GPUs (RTX 4090, A6000, etc.)

# Core Configuration
VLLM_MODEL=openai/gpt-oss-20b
VLLM_PORT=8000
VLLM_HOST=0.0.0.0
VLLM_MAX_MODEL_LEN=40000  # Reduced from 262144 to fit KV cache in GPU memory
VLLM_GPU_MEMORY_UTILIZATION=0.8  # Leaves headroom for other processes
VLLM_TENSOR_PARALLEL_SIZE=1

# Model Loading
VLLM_TRUST_REMOTE_CODE=false  # Trust remote code (set to true for models that require it)
VLLM_DTYPE=auto  # Optional: vLLM --dtype (e.g., auto, float16, bfloat16)

# Function Calling
VLLM_ENABLE_AUTO_TOOL_CHOICE=true  # Enable auto tool choice for function calling
VLLM_TOOL_CALL_PARSER=hermes  # Tool call parser (hermes, mistral, qwen3_xml, openai, gigachat3)

# Model-specific Options (for certain models)
# VLLM_TOKENIZER_MODE=  # mistral for Mistral models
# VLLM_CONFIG_FORMAT=  # mistral for Mistral models
# VLLM_LOAD_FORMAT=  # mistral for Mistral models
VLLM_SPECULATIVE_CONFIG=  # Optional: JSON for vLLM --speculative-config (e.g. MTP settings)

# Pooling Model Options (for embedding/reranker/guard models, vLLM 0.15.0+)
# VLLM_TASK=embed  # Task type: embed, score, classify
# VLLM_RUNNER=pooling  # Runner type: auto, generate, pooling
# VLLM_HF_OVERRIDES=  # JSON string (e.g., for BGE-M3: '{"architectures": ["BgeM3EmbeddingModel"]}')

# CPU Offloading (DEPRECATED in vLLM v1)
VLLM_CPU_OFFLOAD_GB=0  # Use VLLM_KV_OFFLOADING_* instead

# LMCache KV Cache Offloading (vLLM v1+)
# Offloads KV cache to CPU/disk instead of model weights (more efficient for long contexts)
# VLLM_KV_OFFLOADING_BACKEND=lmcache  # Backend for KV cache offloading (e.g., 'lmcache')
# VLLM_KV_OFFLOADING_SIZE=5.0  # Size in GB for KV cache offloading
# VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER=true  # Required when using LMCache

# Model Download Configuration
MODEL_DOWNLOAD_DIR=
HF_CACHE_DIR=

# Logging
LOG_LEVEL=INFO
